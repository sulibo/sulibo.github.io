[ { "title": "A Short Tutorial on Building a Digit Recognizer Using CNN", "url": "/posts/digit-recognizer-cnn/", "categories": "Tech", "tags": "machine learning, deep learning, convolutional neural network, computer vision, tensorflow, Keras", "date": "2020-11-25 23:05:53 +0100", "snippet": "This is a brief tutorial on developing a digit recognizer using CNN and MNIST Data. The accompanying notebook can be found here.Define the problemThe goal here is to correctly identify digits from a dataset of tens of thousands of handwritten images. The MNIST dataset is used for training and testing.Before going forward, the necessary modules need to be imported.# data analysisimport numpy as np import pandas as pd # visualization import matplotlib.pyplot as plt import seaborn as sns sns.set(style='white', context='notebook', palette='deep')# machine learning import sklearnfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import confusion_matrix import itertools#import kerasfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.utils import to_categorical # convert to one-hot-encodingfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D from tensorflow.keras.optimizers import Adamfrom tensorflow.keras.optimizers import SGDfrom tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.callbacks import ReduceLROnPlateau # utilsimport warnings warnings.filterwarnings('ignore')%matplotlib inlineAcquire the dataThe MNIST data can be loaded using Keras.# Load the data from tensorflow.keras.datasets import mnist(X_train, Y_train), (X_test, Y_test) = mnist.load_data()# Plot some examplesfor i in range(9): plt.subplot(3,3,i+1) plt.imshow(X_train[i], cmap=plt.get_cmap('gray'));Prepare the dataData Overviewprint('Train: X=%s, y=%s' %(X_train.shape, Y_train.shape))print('Test: X=%s, y=%s' %(X_test.shape, Y_test.shape))sns.countplot(Y_train);Train: X=(60000, 28, 28), y=(60000,)Test: X=(10000, 28, 28), y=(10000,)The training dataset contains 60000 examples, while the test dataset contains 10000 exmpales. Each example is a 28x28 pixels figure.In addition, we have simlar counts for the 10 digits.Normalization# Normalize the data X_train = X_train / 255.0 X_test = X_test / 255.0 # Encode lables to one hot vectors (ex: 2 -&gt; [0,0,1,0,0,0,0,0,0,0])Y_train = to_categorical(Y_train, num_classes = 10)Y_test = to_categorical(Y_test, num_classes = 10)Reshape the dataX_train = X_train.reshape((X_train.shape[0], 28, 28, 1)) # X_train shape M (number of samples) x H (Height) x W (Width) x D (Depth)X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))Split the datasetX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1)Define the modelWe use the Keras API to build a Convolutional Neural Network model to reconginze the digits in the figures.# Set the CNN model # In -&gt; Conv2D -&gt; relu -&gt; MaxPool2D -&gt; Conv2D -&gt; relu -&gt; Conv2D -&gt; relu -&gt; MaxPool2D -&gt; Flatten -&gt; Dense -&gt; Dense -&gt; Outmodel = Sequential()model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))model.add(MaxPool2D((2, 2)))model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))model.add(MaxPool2D((2, 2)))model.add(Flatten())model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))model.add(Dense(10, activation='softmax'))#opt = Adam(lr=0.01)opt = SGD(lr=0.01, momentum=0.9)model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])model.summary()Model: \"sequential\"_________________________________________________________________Layer (type) Output Shape Param # =================================================================conv2d (Conv2D) (None, 26, 26, 32) 320 _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 13, 13, 32) 0 _________________________________________________________________conv2d_1 (Conv2D) (None, 11, 11, 64) 18496 _________________________________________________________________conv2d_2 (Conv2D) (None, 9, 9, 64) 36928 _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64) 0 _________________________________________________________________flatten (Flatten) (None, 1024) 0 _________________________________________________________________dense (Dense) (None, 100) 102500 _________________________________________________________________dense_1 (Dense) (None, 10) 1010 =================================================================Total params: 159,254Trainable params: 159,254Non-trainable params: 0_________________________________________________________________Train the modelhistory = model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_data=(X_val, Y_val), verbose =1)Epoch 1/101688/1688 [==============================] - 28s 17ms/step - loss: 0.1290 - accuracy: 0.9600 - val_loss: 0.0554 - val_accuracy: 0.9815Epoch 2/101688/1688 [==============================] - 28s 16ms/step - loss: 0.0440 - accuracy: 0.9863 - val_loss: 0.0485 - val_accuracy: 0.9850Epoch 3/101688/1688 [==============================] - 28s 16ms/step - loss: 0.0289 - accuracy: 0.9906 - val_loss: 0.0393 - val_accuracy: 0.9878Epoch 4/101688/1688 [==============================] - 28s 16ms/step - loss: 0.0205 - accuracy: 0.9933 - val_loss: 0.0306 - val_accuracy: 0.9908Epoch 5/101688/1688 [==============================] - 30s 18ms/step - loss: 0.0155 - accuracy: 0.9952 - val_loss: 0.0356 - val_accuracy: 0.9907Epoch 6/101688/1688 [==============================] - 32s 19ms/step - loss: 0.0117 - accuracy: 0.9962 - val_loss: 0.0362 - val_accuracy: 0.9903Epoch 7/101688/1688 [==============================] - 35s 20ms/step - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0381 - val_accuracy: 0.9910Epoch 8/101688/1688 [==============================] - 35s 21ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.0305 - val_accuracy: 0.9920Epoch 9/101688/1688 [==============================] - 35s 21ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.0333 - val_accuracy: 0.9920Epoch 10/101688/1688 [==============================] - 36s 21ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0366 - val_accuracy: 0.9918The training accuracy is 0.9990, and the validation accuracy is 0.9918.# Save the modelmodel.save('cnn_model.h5')Evulate the model# predict resultsloss, accuracy = model.evaluate(X_test, Y_test, verbose=1)313/313 [==============================] - 2s 5ms/step - loss: 0.0325 - accuracy: 0.9919The accuracy on the test dataset is 0.9919.# Show the first few predictionsfor i in range(9): ax = plt.subplot(3,3,i+1) results = model.predict(X_test[i:i+1,:,:,:]) plt.tight_layout() predicted_digit = np.argmax(results, axis = 1) plt.imshow(X_test[i,:,:,0],cmap=plt.get_cmap('gray')); ax.set_title(\"Prediction: %s\" %predicted_digit[0],fontsize=16)ReferencesThis tutorial has been created based on great work done solving this digit recognizer problem. https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/ https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6" }, { "title": "Yet Another Kaggle Titanic Competition Tutorial", "url": "/posts/kaggle-titanic-competition-with-deep-neural-network-using-keras/", "categories": "Tech", "tags": "machine learning, deep learning, tensorflow, Keras, Kaggle", "date": "2020-11-23 21:09:00 +0100", "snippet": "This post is a tutorial on solving the Kaggle Titanic Competition using Deep Neural Network with the TensorFlow API Keras. Kaggle is a competition site which provides problems to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The Titanic competition is probably the first competition you will come across on Kaggle. The goal of the competition is to predict the possibility of survival of the passengers onboard, a typical classification problem.This tutorial follows a typical workflow of machine learning project. Define the problem Acquire the data Prepare the data Define the model Train and fine-tune the model Test and deploy the modelThe accompanying notebook of this tutorial is available here.Define the problemThe question or problem definition for Titanic Survival competition is described here at Kaggle. Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.Kaggle already provides some early understanding about the domain of this problem, see the Kaggle competition description page here. Here are the highlights to note. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.Before going forward, we need to import some necessary modules.# data analysisimport numpy as np import pandas as pd # visualization import matplotlib.pyplot as plt import seaborn as sns # machine learning import sklearnfrom sklearn.model_selection import train_test_splitimport tensorflow as tffrom tensorflow.python.framework import ops# utilsimport warnings warnings.filterwarnings('ignore')%matplotlib inlineAcquire the dataThe Python Pandas packages are used to acquire the data. The data consists of two groups: a training set (train.csv)and test set (test.csv).The training set should be used to build the machine learning models. For the training set, the outcome (also known as the “ground truth”) for each passenger is provided.The test set should be used to see how well your model performs on unseen data. For the test set, the ground truth for each passenger is not provided. The machine learning models will be used to predict the surival of each passenger.train_data = pd.read_csv('./Dataset/train.csv')test_data = pd.read_csv('./Dataset/test.csv')train_data.head() PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S test_data.head() PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 892 3 Kelly, Mr. James male 34.5 0 0 330911 7.8292 NaN Q 1 893 3 Wilkes, Mrs. James (Ellen Needs) female 47.0 1 0 363272 7.0000 NaN S 2 894 2 Myles, Mr. Thomas Francis male 62.0 0 0 240276 9.6875 NaN Q 3 895 3 Wirz, Mr. Albert male 27.0 0 0 315154 8.6625 NaN S 4 896 3 Hirvonen, Mrs. Alexander (Helga E Lindqvist) female 22.0 1 1 3101298 12.2875 NaN S Prepare the dataThis step aims to prepare the data for use in machine learning training. It involves cleaning up the data, understanding its structure and attributes, studying possible correlations between attributes, and visualizing the data to extract human-level insight on its properties, distribution, range, and overall usefulness [3].Data Overviewtrain_data.info()print(\"-\" * 40)test_data.info()&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5)memory usage: 83.7+ KB----------------------------------------&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 418 entries, 0 to 417Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 417 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5)memory usage: 36.0+ KBObservations The data contains 11 features. The details of these features are described on the Kaggle data pape here. Categorical: Sex and Embarked, Orinial: Pclass, Numerical: Age and Fare (continous), SibSp and Parch (discret), Mixed: Cabin (alphanumeric). There are missing values in Age, Cabin, Embarked and Fare.Primary data analysis and visualization# Overall Surival Ratetrain_data['Survived'].value_counts().plot.pie(autopct = '%1.2f%%');Sex# Sex train_data[['Sex','Survived']].groupby(['Sex']).mean().plot.bar();Observation: Female passengers had higher survival chance.Class# Class train_data[['Pclass', 'Survived']].groupby('Pclass').mean().plot.bar();Observation: Surival rate Class 1 &gt; Class 2 &gt; Class 3Family size# Family Size train_data['FamilySize'] = train_data['Parch'] + train_data['SibSp'] + 1 test_data['FamilySize'] = test_data['Parch'] + test_data['SibSp'] + 1 sns.barplot(data=train_data, x='FamilySize', y='Survived');Observation: Passengers with small family size (2-4) were more likely to survive.Therefore, the passengers are grouped into 3 groups according the size of family: singleton, SmallFamily (family size 2-4) and LargeFamily (family size larger than 4).# Feature engineeringtrain_data['Singleton'] = train_data['FamilySize'].map(lambda s: 1 if s == 1 else 0)train_data['SmallFamily'] = train_data['FamilySize'].map(lambda s: 1 if 2&lt;= s &lt;= 4 else 0)train_data['LargeFamily'] = train_data['FamilySize'].map(lambda s: 1 if s&gt;=5 else 0)test_data['Singleton'] = test_data['FamilySize'].map(lambda s: 1 if s == 1 else 0)test_data['SmallFamily'] = test_data['FamilySize'].map(lambda s: 1 if 2&lt;= s &lt;= 4 else 0)test_data['LargeFamily'] = test_data['FamilySize'].map(lambda s: 1 if s&gt;=5 else 0)Age# Age &amp; Survival Rate age_data = train_data[train_data['Age'].notna()][['Age', 'Survived']]sns.histplot(age_data['Age']);ageFacet=sns.FacetGrid(train_data,hue='Survived',aspect=3);ageFacet.map(sns.kdeplot,'Age',shade=True);ageFacet.set(xlim=(0,train_data['Age'].max()));ageFacet.add_legend();Observation: Passengers aged under 10 were more likely to survive.Fare# Fare &amp; Survival RatefareFacet=sns.FacetGrid(train_data,hue='Survived',aspect=3);fareFacet.map(sns.kdeplot,'Fare',shade=True);fareFacet.set(xlim=(0,150));fareFacet.add_legend();Observation: Low survival rate for fare &lt; 20.Embarked port# Embarked &amp; Survival Ratesns.barplot(data=train_data,x='Embarked',y='Survived');Observations: Passengers embarked from port C were more likely to survive.Name (Title)The titles of the passengers can be extracted from their names. Passengers from different social classes may have different survival chance.# Extract titlestrain_data['Title'] = train_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)test_data['Title'] = test_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)train_data['Title'].value_counts()Mr 517Miss 182Mrs 125Master 40Dr 7Rev 6Mlle 2Major 2Col 2Ms 1Don 1Lady 1Mme 1Sir 1Countess 1Capt 1Jonkheer 1Name: Title, dtype: int64# Group the passengers into 6 groups by their titles.Title_Dictionary = { \"Capt\": \"Officer\", \"Col\": \"Officer\", \"Major\": \"Officer\", \"Jonkheer\": \"Royalty\", \"Don\": \"Royalty\", \"Sir\" : \"Royalty\", \"Dr\": \"Officer\", \"Rev\": \"Officer\", \"Countess\": \"Royalty\", \"Dona\": \"Royalty\", \"Mme\": \"Mrs\", \"Mlle\": \"Miss\", \"Ms\": \"Mrs\", \"Mr\" : \"Mr\", \"Mrs\" : \"Mrs\", \"Miss\" : \"Miss\", \"Master\" : \"Master\", \"Lady\" : \"Royalty\" }train_data['Title']=train_data['Title'].map(Title_Dictionary)test_data['Title']=test_data['Title'].map(Title_Dictionary)train_data['Title'].value_counts()Mr 517Miss 184Mrs 127Master 40Officer 18Royalty 5Name: Title, dtype: int64sns.barplot(data=train_data, x='Title', y='Survived');Observation: Passengers with “Mr” and “Officer” titles had low survival chance.CabinThe Cabin information can tell which decks the passengers were resided, which may influence the survival rate. The Cabin variables have many missing values. THe missing values will be replaced by “Unknown”.train_data['Cabin'] = train_data['Cabin'].fillna('Unknown')test_data['Cabin'] = test_data['Cabin'].fillna('Unknown')# Cabin train_data['Deck']=train_data['Cabin'].map(lambda x:x[0])test_data['Deck']=test_data['Cabin'].map(lambda x:x[0])sns.barplot(data=train_data, x='Deck', y='Survived');Observation: Passengers in Deck B, D, E were more likely to survive.Data pre-processingCombine all datafull_data = pd.concat([train_data, test_data], axis=0)Fill the missing valuesEmbarked# Embarkedfull_data[full_data['Embarked'].isnull()] PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked FamilySize Singleton SmallFamily LargeFamily Title Deck 61 62 1.0 1 Icard, Miss. Amelie female 38.0 0 0 113572 80.0 B28 NaN 1 1 0 0 Miss B 829 830 1.0 1 Stone, Mrs. George Nelson (Martha Evelyn) female 62.0 0 0 113572 80.0 B28 NaN 1 1 0 0 Mrs B full_data['Embarked'].value_counts()S 914C 270Q 123Name: Embarked, dtype: int64Fill the missing value of the Embarked feature with the most frequent value.full_data['Embarked'] = full_data['Embarked'].fillna('S')Fare# Fare full_data[full_data['Fare'].isnull()] PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked FamilySize Singleton SmallFamily LargeFamily Title Deck 152 1044 NaN 3 Storey, Mr. Thomas male 60.5 0 0 3701 NaN Unknown S 1 1 0 0 Mr U Fill the missing value of the Fare feature with the mean value.full_data['Fare']=full_data['Fare'].fillna(full_data[(full_data['Pclass']==3)&amp;(full_data['Embarked']=='S')&amp;(full_data['Cabin']=='Unknown')]['Fare'].mean())AgeThe Random Forest algorithm is used to fill the missing of the Age feature.AgePre = full_data[['Age','Pclass','Title','FamilySize','Sex']]AgePre = pd.get_dummies(AgePre)AgePre.head() Age Pclass FamilySize Title_Master Title_Miss Title_Mr Title_Mrs Title_Officer Title_Royalty Sex_female Sex_male 0 22.0 3 2 0 0 1 0 0 0 0 1 1 38.0 1 2 0 0 0 1 0 0 1 0 2 26.0 3 1 0 1 0 0 0 0 1 0 3 35.0 1 2 0 0 0 1 0 0 1 0 4 35.0 3 1 0 0 1 0 0 0 0 1 # Split the datesetAgeKnown=AgePre[AgePre['Age'].notnull()]AgeUnKnown=AgePre[AgePre['Age'].isnull()]AgeKnown_X=AgeKnown.drop(['Age'],axis=1)AgeKnown_y=AgeKnown['Age']AgeUnKnown_X=AgeUnKnown.drop(['Age'],axis=1)# Setup the random forest modelfrom sklearn.ensemble import RandomForestRegressorrfr=RandomForestRegressor(random_state=None,n_estimators=500,n_jobs=-1)rfr.fit(AgeKnown_X,AgeKnown_y)# Model evaluationrfr.score(AgeKnown_X,AgeKnown_y)0.4911278076281459# Predict ageAgeUnKnown_y=rfr.predict(AgeUnKnown_X)# Fill the mssing valuefull_data.loc[full_data['Age'].isnull(),['Age']]=AgeUnKnown_y# Create featuer IsChild (less than 10 years old)full_data['IsChild'] = full_data['Age'].map(lambda s: 1 if s &lt;= 10 else 0)full_data.info() # no missing values now (except Survived)&lt;class 'pandas.core.frame.DataFrame'&gt;Int64Index: 1309 entries, 0 to 417Data columns (total 19 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 1309 non-null int64 1 Survived 891 non-null float64 2 Pclass 1309 non-null int64 3 Name 1309 non-null object 4 Sex 1309 non-null object 5 Age 1309 non-null float64 6 SibSp 1309 non-null int64 7 Parch 1309 non-null int64 8 Ticket 1309 non-null object 9 Fare 1309 non-null float64 10 Cabin 1309 non-null object 11 Embarked 1309 non-null object 12 FamilySize 1309 non-null int64 13 Singleton 1309 non-null int64 14 SmallFamily 1309 non-null int64 15 LargeFamily 1309 non-null int64 16 Title 1309 non-null object 17 Deck 1309 non-null object 18 IsChild 1309 non-null int64 dtypes: float64(3), int64(9), object(7)memory usage: 204.5+ KBfull_data.head() PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked FamilySize Singleton SmallFamily LargeFamily Title Deck IsChild 0 1 0.0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 Unknown S 2 0 1 0 Mr U 0 1 2 1.0 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 0 1 0 Mrs C 0 2 3 1.0 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 Unknown S 1 1 0 0 Miss U 0 3 4 1.0 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 2 0 1 0 Mrs C 0 4 5 0.0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 Unknown S 1 1 0 0 Mr U 0 Select the features# Remove the less revelant featuresfullSel=full_data.drop(['Cabin','Name','Ticket','PassengerId'],axis=1)# Check the correlations of the featurescorrDf=pd.DataFrame()corrDf=fullSel.corr()corrDf['Survived'].sort_values(ascending=True)Pclass -0.338481Singleton -0.203367LargeFamily -0.125147Age -0.065783SibSp -0.035322FamilySize 0.016639Parch 0.081629IsChild 0.125678Fare 0.257307SmallFamily 0.279855Survived 1.000000Name: Survived, dtype: float64# Heatmapplt.figure(figsize=(8,8));sns.heatmap(fullSel[['Survived','Age','Embarked','Fare','Parch','Pclass', 'Sex','SibSp','Title','Singleton','SmallFamily','LargeFamily','FamilySize','IsChild','Deck']].corr(),cmap='BrBG',annot=True, linewidths=.5);plt.xticks(rotation=45);fullSel=fullSel.drop(['SibSp','Parch','FamilySize','Age'],axis=1)One-hot encoding# One-hot encodingfullSel=pd.get_dummies(fullSel)PclassDf=pd.get_dummies(full_data['Pclass'],prefix='Pclass')PclassDf.head() Pclass_1 Pclass_2 Pclass_3 0 0 0 1 1 1 0 0 2 0 0 1 3 1 0 0 4 0 0 1 fullSel = pd.concat([fullSel, PclassDf], axis=1)fullSel.drop('Pclass', axis=1, inplace=True)fullSel.head() Survived Fare Singleton SmallFamily LargeFamily IsChild Sex_female Sex_male Embarked_C Embarked_Q ... Deck_C Deck_D Deck_E Deck_F Deck_G Deck_T Deck_U Pclass_1 Pclass_2 Pclass_3 0 0.0 7.2500 0 1 0 0 0 1 0 0 ... 0 0 0 0 0 0 1 0 0 1 1 1.0 71.2833 0 1 0 0 1 0 1 0 ... 1 0 0 0 0 0 0 1 0 0 2 1.0 7.9250 1 0 0 0 1 0 0 0 ... 0 0 0 0 0 0 1 0 0 1 3 1.0 53.1000 0 1 0 0 1 0 0 0 ... 1 0 0 0 0 0 0 1 0 0 4 0.0 8.0500 1 0 0 0 0 1 0 0 ... 0 0 0 0 0 0 1 0 0 1 5 rows × 29 columnsNormalizationfullSel[fullSel.columns] = fullSel[fullSel.columns].apply(lambda x: x/x.max(), axis=0)fullSel.head() Survived Fare Singleton SmallFamily LargeFamily IsChild Sex_female Sex_male Embarked_C Embarked_Q ... Deck_C Deck_D Deck_E Deck_F Deck_G Deck_T Deck_U Pclass_1 Pclass_2 Pclass_3 0 0.0 0.014151 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 1 1.0 0.139136 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0 ... 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 2 1.0 0.015469 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 3 1.0 0.103644 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 ... 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 4 0.0 0.015713 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 5 rows × 29 columnsfullSel.to_csv('combined_data.csv', index=False)Split the datasettrain = fullSel[fullSel['Survived'].notnull()]test = fullSel[fullSel['Survived'].isnull()]targets = train['Survived']train.drop('Survived', axis=1, inplace=True)test.drop('Survived', axis=1, inplace=True)X_train,X_val,Y_train,Y_val = train_test_split(train,targets,test_size = 0.2,random_state = 42)Define the modelA neural network model with 3 layers are chosen. The model is implemented using Keras.x = X_trainy = Y_trainL1=20L2=20L3=5model = tf.keras.Sequential()model.add(tf.keras.layers.Dense(L1,input_shape=(X_train.shape[1],),kernel_regularizer='l2', activation='relu'))model.add(tf.keras.layers.Dense(L2,kernel_regularizer='l2', activation='relu'))model.add(tf.keras.layers.Dense(L3,kernel_regularizer='l2', activation='relu'))model.add(tf.keras.layers.Dense(1,kernel_regularizer='l2', activation='sigmoid'))model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])Train and fine-tune modelepoch = 50history = model.fit(x,y,epochs=epoch,batch_size=32,verbose=0)preds = model.evaluate(x=X_val, y=Y_val)print (\"Loss = \" + str(preds[0]))print (\"Test Accuracy = \" + str(preds[1]))print (\"Training Accuracy = \" + str(history.history['accuracy'][-1]))Output:Loss = 0.5216637253761292Test Accuracy = 0.826815664768219Training Accuracy = 0.8300561904907227Observation: Test accuracy and training accuracy are close, indicating that the model is not over-fitting.Retrain the model with all training data.history = model.fit(train,targets,epochs=epoch,batch_size=32,verbose=0)preds = model.evaluate(x=X_val, y=Y_val)print (\"Loss = \" + str(preds[0]))print (\"Accuracy = \" + str(history.history['accuracy'][-1]))Output:Loss = 0.49221670627593994Accuracy = 0.8316498398780823Test and deploy modell = model.predict(test)survived = [1 if x &gt; 0.5 else 0 for x in l]#original_test_data = pd.read_csv('/kaggle/input/titanic/test.csv')original_test_data = pd.read_csv('./Dataset/test.csv')submission = pd.DataFrame({ 'PassengerId': original_test_data['PassengerId'], 'Survived': survived})submission.to_csv('titanic-nn.csv', index=False)Submission of this result on Kaggle results in a public score of 0.77751, ranking 4428th out of 16823 at the time of writing.ReferencesThis tutorial has been created based on great work done solving the Titanic competition and machine learning. https://zhuanlan.zhihu.com/p/50194676 https://www.kaggle.com/startupsci/titanic-data-science-solutions https://www.kdnuggets.com/2020/09/mathworks-deep-learning-workflow.html " }, { "title": "Kalman filter", "url": "/posts/kalman_filter/", "categories": "Tech", "tags": "control engineering, autonomous driving", "date": "2020-07-20 17:51:18 +0200", "snippet": "Kalman fiter is one of the most important and widely used estimation algorithms. When taking the Self-Driving Cars courses on Coursera, I find the introduction of the Kalman filter in the course a bit brief. So I decided to write this post to provide more information on it. From a control perspective, the Kalman filter is similar to a state observer. It uses both prediction and the measurement to provide an optimal estimate of the states amid the uncertainties/noises in the model and measurements.State ObserverLet us start with a short introduction to state observer. For a dynamical system, it is not always possible to obtain information on all state variables in practice. In this case, a state observer is designed to estimate the states of the system and then will be used for control design.Consider a linear time invariant system, represented in the state space form as below\\[\\begin{equation}\\begin{aligned}x_k &amp;=Ax_{k-1}+Bu_k, \\\\y_k &amp;=Cx_k,\\end{aligned}\\label{equ:discrete_time_system}\\end{equation}\\]where $x_k\\in\\mathcal{R}^n$, $u_k\\in\\mathcal{R}^m$, $y_k\\in\\mathcal{R}^p$ are the state, input and output of the system at time step $k$.Given the model ($A,B,C$) and the measurements ($y_k$), a state observer is given by the following equations\\[\\begin{equation}\\begin{aligned}\\hat{x}_k &amp;= A\\hat{x}_{k-1}+Bu_{k-1}+K(y_k-\\hat{y}_k),\\\\\\hat{y}_k &amp;= C\\hat{x}_k,\\end{aligned}\\label{equ:observer}\\end{equation}\\]where \\(\\hat{x}_{k}\\) is the estimate of the state $x_k$, and $K$ is gain of the observer to be designed.The left-hand side of the first equation in \\eqref{equ:observer} can be divided into: a prediction part \\(A\\hat{x}_{k-1}+Bu_{k-1}\\) calculated based on the state estimate and input at the previous step, a correction part $K(y_k-\\hat{y}_k)$ calculated based on the difference of the measured and predicted outputs.The configuration of a state observer is shown in the following figure.The goal of the state observer is to provide an accurate estimate of the state, which is done by choosing a proper observer gain $K$ such that the estimation error $e_k=x_k-\\hat{x}_k$ converges to zero.It follows from \\eqref{equ:discrete_time_system} and \\eqref{equ:observer} that\\[\\begin{equation}e_{k}=(I-KC)e_{k-1}.\\label{equ:error}\\end{equation}\\]Then the observer gain $K$ should be chosen such that the system \\eqref{equ:error} is stable.Kalman filterHowever, in reality, we will need to consider the noises/uncertainties in the model and the measurement sensors. The Kalman filter is designed to handle this situation.Taking into the noises into account, the system \\eqref{equ:discrete_time_system} is transformed into:\\[\\begin{equation}\\begin{aligned}x_k &amp;=x_{k-1}+Bu_{k-1}+w_{k-1}\\\\y_k &amp;=Cx_k+v_k,\\end{aligned}\\label{equ:discrete_time_system_noise}\\end{equation}\\]where $w_k\\sim \\mathcal{N}(0,Q)$ is the process noise, and $v_k\\sim \\mathcal{N}(0,R)$ is the measurement noise. These noises are assumed to be independent (of each other), white and noramally distributed.The Kalman filter is given by\\[\\begin{equation}\\begin{aligned}\\hat{x}_k &amp;=A\\hat{x}_{k-1}+Bu_{k-1}+K(y_k-\\hat{y})\\\\\\hat{y}_k &amp;=C(A\\hat{x}_{k-1}+Bu_{k-1})\\end{aligned}\\label{equ:kalman_filter}\\end{equation}\\]The configuration of the Kalman filter is shown below.Now let us introduce some concepts in the context of Kalman filter. Priori estimate: \\(\\hat{x}_k^-=A\\hat{x}_{k-1}+Bu_{k-1}\\). Posterior estimate: \\(\\hat{x}_k=A\\hat{x}_{k-1}+Bu_{k-1}+K(y_k-\\hat{y}_k)=\\hat{x}_k^-+K(y_k-C\\hat{x}_k^-)\\). Priori estimate error: $e_k^-=x_k-\\hat{x}^-_k$. Posterior estimate error: $e_k = x_k -\\hat{x}_k$. Priori estimate error covariance: $P_k^-=E[e_k^-e_k^{-\\top}]$ Posterior estimate error covariance: $P_k=E[e_ke_k^\\top]$The Kalman filter choose a $K$ known as the Kalman filter gain such that the posteriori estimate $\\hat{x}_k$ is optimal in sense that the variance of the posteriori estimate error is minimal. In addition, the posteriori estimate $\\hat{x}_k$ is an unbiased estimate of $x_k$.Unbiased estimateThe expected value of the posteriori estimate is given by\\[\\begin{equation}\\begin{aligned}E[\\hat{x}_k]&amp;=E[\\hat{x}_k^- +K(y_k-C\\hat{x}_k^-)]\\\\ &amp;=E[\\hat{x}_k^-]+K(E[Cx_k]+E[v_k]-E[C\\hat{x}_k^-])\\\\ &amp;=E[\\hat{x}_k^-]+KC(E(x_k)-E[\\hat{x}_k^-]).\\end{aligned}\\end{equation}\\]Setting the initial estimate to be the initial value leads to\\[E[\\hat{x}_k^-]=E[x_k],\\]which indicates\\[E[\\hat{x}_k]=E[x_k].\\]So the Kalman filter always provides an unbiased estimate of the states regardless of the Kalman filter gain.Minimum varianceThe Kalman filter gain $K$ is chosen such that the variance of the posteriori error $e_k$ is minimal.From \\eqref{equ:kalman_filter}, we have\\[\\begin{equation}e_k=x_k-\\hat{x}_k=x_k-\\hat{x}_k^- - K(Cx_k+v_k-C\\hat{x}_k^-).\\end{equation}\\]Take the covariance of $e_k$, yields\\[\\begin{equation}P_k=E[e_ke_k^\\top]=(I-KC)P_k^- (I-KC)^\\top + KRK^\\top.\\end{equation}\\]That is\\[\\begin{equation}P_k=P_k^- -KCP_k^- - P_k^- C^\\top K^\\top + K(CP_k^-C+R)K^\\top.\\label{equ:Pk_Pk_minus}\\end{equation}\\]Then the Kalman filter design problem become an optimal problem\\[\\begin{equation}\\min_{K}\\mathrm{tr} (P_k)\\end{equation}\\]Note that for any matrix \\(M\\) and a symmetric matrix $N$,\\[\\begin{equation*}\\frac{\\partial \\mathrm{tr}(MNM^\\top)}{\\partial M}=2MN.\\end{equation*}\\]Then differentiating $\\mathrm{tr} (P_k)$ with respect to $K$ and settting to zero, yields\\(\\begin{equation}\\frac{\\partial \\mathrm{tr} (P_k)}{\\partial K}=-2(P_k^-C^\\top)+2K(CP_k^-C^\\top+R)=0.\\label{equ:trP}\\end{equation}\\)Solving the equation above for $K$, we have\\[\\begin{equation}K=P_k^-C^\\top(CP_k^-C^\\top+R)^{-1}.\\label{equ:kalman_filter_gain}\\end{equation}\\]Substitute \\eqref{equ:kalman_filter_gain} into \\eqref{equ:Pk_Pk_minus}, yields\\[\\begin{equation}P_k=(I-KC)P_k^-.\\end{equation}\\]For $P_k^-$, we have\\[\\begin{equation}\\begin{aligned}P_k^-&amp;=E[e^-_k e_k^{- \\top}]=E[(x_k-x_k^-)(x_k-x_k^-)^\\top]\\\\ &amp;=E[(Ae_{k-1}+w_{k-1})(Ae_{k-1}+w_{k-1})^\\top]\\\\ &amp;=AP_{k-1}A^\\top+Q.\\end{aligned}\\end{equation}\\]Kalman filter processNow, let us put everything together to form the Kalman filter algorithm. The algorithm consists of two parts: Predict Project the state ahead Project the error covariance ahead Update Compute the Kalman filter gain Update estimate with measurement Update the error covariance " }, { "title": "Some Handy Software Tools", "url": "/posts/some-handy-software-tools/", "categories": "", "tags": "", "date": "2017-12-07 16:20:36 +0100", "snippet": "Here are some software tools I used which are handy for my daily use of laptops.Vim Plug Managements: vim-plug Plugins: Vim-Latex Vim-mucomplete Nerdtree Vim-hybrid-material vim-airline vim-airline-themes vim-fugitive vimwiki calendar.vim ctrlp.vim vim-matlab VOoM vim-livedown LaTeX LaTeX tutorial: LaTeX notes (Chinese) Tikz Examples: TeXample.net Editor: TexStudioMATLAB matlab2tikzReference Managements Zotero Mendeley Jabref" }, { "title": "Octopress 3 Tutorial", "url": "/posts/octopress-3-tutorial/", "categories": "Tech", "tags": "jekyll, blog, github-page, octopress", "date": "2017-10-07 09:50:20 +0200", "snippet": "Octopress is a blog toolkit built on top of Jekyll. Using Octopress, we can easily build a static blog website from scratch. Recently, Octopress 3.0 has been released on its GitHub page. Octopress 3.0 is a full rewrite of Octopress 2. This version is now only a plugin for Jekyll so no division between Octopress and Jekyll. The management is cleaner than the previous versions. However, the official documents are not complete yet. So I decided to write a tutorial for it, including installation, basic use, deployment to Github page, and also howto use Octopress from different places. InstallPreparationTwo packages are required before installing Octopress: Ruby and Git.In Arch Linux, you can install these two software by the following commands in your terminal.$ sudo pacman -S ruby $ sudo pacman -S gitFor other Linux distributions, use the corresponding package managers to install these two packages. For example, for Ubuntu user, use the following commands.$ sudo apt-get install ruby $ sudo apt-get install git Install OctopressWith Ruby and Git installed, you can easily install Octopress.$ gem install octopressOr you can use Bundler. Bundler can be installed by Gem.$ gem install bundlerCreate a fine named ‘Gemfile’ in your site’s root directory with the following content:source 'https://rubygems.org'gem 'octopress', '~&gt;3.0'And then run$ bundleSetupWith the new Octopress CLI commands, seting up your blog site is easy.First, navigate to your working directory and run the following command:$ octopress new your_blog_site_nameOctopress will generate a directory named your_blog_site_name with scaffolding for the website.your_blog_site_name.│___ _config.yml|___ .gitignore|___ .sass-cache|___ about.md|___ Gemfile|___ Gemfile.lock|___ index.md|___ _posts| |___ 2017-10-07-welcome-to-jekyll.makrdown |___ _templates |___ draft |___ page |___ postYou will need to configure the setting for your website. The configuration file is _config.yml located in the root directory of the site. Below is an example of such file. In this file, you can add the information of your site (title, url, social accouts, etc.).# Welcome to Jekyll!## This config file is meant for settings that affect your whole blog, values# which you are expected to set up once and rarely edit after that. If you find# yourself editing this file very often, consider using Jekyll's data files# feature for the data you need to update frequently.## For technical reasons, this file is *NOT* reloaded automatically when you use# 'bundle exec jekyll serve'. If you change this file, please restart the server process.# Site settings# These are used to personalize your new site. If you look in the HTML files,# you will see them accessed via Libo Su, , and so on.# You can create any custom variable you would like, and they will be accessible# in the templates via .title: Your awesome titleemail: your-email@example.comdescription: &gt;- # this means to ignore newlines until \"baseurl:\" Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.baseurl: \"\" # the subpath of your site, e.g. /blogurl: \"\" # the base hostname &amp; protocol for your site, e.g. http://example.comtwitter_username: jekyllrbgithub_username: jekyll# Build settingsmarkdown: kramdowntheme: minimaplugins: - jekyll-feed# Exclude from processing.# The following items will not be processed, by default. Create a custom list# to override the default setting.# exclude:# - Gemfile# - Gemfile.lock# - node_modules# - vendor/bundle/# - vendor/cache/# - vendor/gems/# - vendor/ruby/Basic UsageCreate a PostOctopress provides the following Octopress CLI command to create a post$ octopress new post Hello World This command create a new blog post with the tile of Hello World. You can change the title to any title you like. The newly created post can be found in the _post directory, which looks like below:---layout: post title: \"Hello World\"---The post file is a markdown file by default. You can add you content below the second ---. The lines between the two triple-hyphen --- is the YAML front matter, where you can set predefined and custom variables, such as Categories, Tags and permalink for the post. More details can be found in Jekyll Documentation.Build and Preview the siteOctopress use Jekyll commands for building and preview the site. We can use the following command to build:$ jekyll build This command will build a static HTML website in the _site directory.To preview the site built, use the command below$ jekyll serve Use you favorite browser to view it at the address localhost: 4000.DeployOctopress includes a deploy module, called octopress deploy that allows your site to be deployed to Rsync, S3 or GitHub papges. I deploy my site on GitHub page and the required steps are listed below.First, create a repository named username.github.io on your Github. Your blog can be accessed via “https://username.github.io” after deployment. The “username” should be your own username.Then, set up the deployment configuration by octopress deploy.$ octopress deploy init git git@github.com:username/username.github.ioRemember to replace “username” with your own username.Update the _config.yml if necessary (for example, add your blog URL). The line start with “url” in _config.yml should beurl: \"https://username.github.io\" # the base hostname &amp; protocal for your site , e.g. http://exampel.com Build your site.$ jekyll buildDeploy your site to GitHub page.$ octopress deploy After this, you can view your site at https://username.github.io.With octopress deploy, a master branch is created in the directory .deploy and pushed to your GitHub page repository.To manage the source code, we can create a source branch and push it to your GitHub page repository.First add the following two lines in your .gitignore file..deploy _deploy.ymlSet your site’s root directory as a Git repository. In your site’s root directory, run$ git initAdd the remote.$ git remote add origin git@github.com:username/username.github.ioCreate the source branch.$ git checkout -b source Push the source code to GitHub.$ git add . $ git commit -m \"Initialization\" $ git push origin source Use in different placesThis section describes how to recreate a local repository of your Octopress blog. This allows you to blog from different places.Before we start, let us have a look at how Octopress works. If you follow the deployment steps in the previous section, your Octopress repository on GitHub have two branches, source and master. The source branch contains the files to generate the blog and the master contains the blog itself. In the local repository, the master branch is stored in the subdirectory named .deploy. With ‘.deploy’ in the .gitignore file, it is ignored when you run git push origin source. The master branch is updated when you run octopress deploy.Clone your blog to the new locationAt the new location, clone the source branch to the local Octopress folder.$ git clone -b source git@github.com:username/username.github.io.git your_blog_site_nameThen install Octopress using Bundler.$ cd your_blog_site_name$ bundle Build the site.$ jekyll buildSet up GitHub pages deployment.$ octopress deploy init git git@github.com:username/username.github.io.gitPull the master branch from the GitHub repository via octopress deploy.$ octopress deployNote that in Octopress 3.0, octopress deploy includes the step of cloning (pulling) the master branch from the remote to the folder .deploy folder.Pushing changes from two different machinesTo blog from more than one computer, you must make sure you push everything before switching between computers. In the first machine, follow the instructions after you have made the changes$ jekyll build $ octopress deploy # update the blog and the remote master branch $ git add . $ git commit -m \"some comments\" $ git push origin source # update the remote source branchIn the second machine, pull the changes$ git pull origin source #update the local source branch After making the changes, update the blog and update the repository again$ jekyll build $ octopress deploy # udpate the blog and the remote master branch $ git add . $ git commit -m \"some comments\" $ git push origin source " } ]
